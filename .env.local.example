# Local development environment — copy to .env.local and fill in values.
#
# Usage:
#   cp .env.local.example .env.local
#   # Edit .env.local with real values
#   docker compose -f docker-compose.local.yml --env-file .env.local up -d

# ---------------------------------------------------------------------------
# Database
# ---------------------------------------------------------------------------
# Choose any password — local dev only, never production
POSTGRES_PASSWORD=wopr_local_dev

# ---------------------------------------------------------------------------
# Auth (better-auth)
# ---------------------------------------------------------------------------
# Generate with: openssl rand -hex 32
BETTER_AUTH_SECRET=REPLACE_ME_generate_with_openssl_rand_hex_32

# ---------------------------------------------------------------------------
# Platform internal auth
# ---------------------------------------------------------------------------
# Used for vault encryption and internal service-to-service auth.
# Generate with: openssl rand -hex 32
# Must be at least 32 characters.
PLATFORM_SECRET=REPLACE_ME_generate_with_openssl_rand_hex_32

# ---------------------------------------------------------------------------
# GPU node
# ---------------------------------------------------------------------------
# Secret the gpu-seeder sends to /internal/gpu/register. Must match GPU_NODE_SECRET
# in the platform-api environment above.
# Generate with: openssl rand -hex 24
GPU_NODE_SECRET=REPLACE_ME_generate_with_openssl_rand_hex_24

# Stable identifier for this local GPU node row in the database.
# Change this if you want to reset the GPU node registration.
GPU_NODE_ID=local-gpu-node-001

# ---------------------------------------------------------------------------
# Stripe (test mode keys — use test keys for local dev)
# ---------------------------------------------------------------------------
STRIPE_SECRET_KEY=sk_test_REPLACE_ME
STRIPE_WEBHOOK_SECRET=whsec_REPLACE_ME
STRIPE_DEFAULT_PRICE_ID=price_REPLACE_ME

# ---------------------------------------------------------------------------
# Email (Resend — optional in local dev)
# ---------------------------------------------------------------------------
# Leave as placeholder to disable email sending; the API will log instead
RESEND_API_KEY=re_placeholder_local_dev
RESEND_FROM_EMAIL=noreply@localhost

# ---------------------------------------------------------------------------
# Model weights — GPU inference
# ---------------------------------------------------------------------------
# Directory on the host containing GGUF model files.
# Assumed to already exist with models downloaded. See GPU.md for model list.
MODELS_PATH=/opt/models

# llama.cpp text generation
# Recommended for RTX 3070 (8 GB VRAM): use a Q4_K_M quantization to fit in VRAM
# along with whisper + qwen. Full Q8_0 (~9 GB) will not fit.
# Example: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB VRAM)
LLAMA_MODEL_FILE=Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
LLAMA_CTX_SIZE=4096
LLAMA_GPU_LAYERS=-1
# Reduce parallel slots on 8 GB VRAM — more slots = more VRAM per slot
LLAMA_PARALLEL=2

# Qwen embeddings
EMBEDDINGS_MODEL_FILE=qwen2-0_5b-instruct-q8_0.gguf
EMBEDDINGS_CTX_SIZE=4096
EMBEDDINGS_POOLING=mean

# Whisper STT (auto-downloaded from HuggingFace on first start)
# Use tiny or small on RTX 3070 to save VRAM
WHISPER_MODEL=Systran/faster-whisper-small

# ---------------------------------------------------------------------------
# Bot image (for Dockerode fleet spawning)
# ---------------------------------------------------------------------------
WOPR_BOT_IMAGE=ghcr.io/wopr-network/wopr:latest

# ---------------------------------------------------------------------------
# GHCR credentials (optional — only needed if pulling private images)
# ---------------------------------------------------------------------------
REGISTRY_USERNAME=
REGISTRY_PASSWORD=
REGISTRY_SERVER=ghcr.io

# ---------------------------------------------------------------------------
# DigitalOcean (optional in local dev)
# ---------------------------------------------------------------------------
# Only needed if you want InferenceWatchdog to attempt droplet reboots.
# Without this, watchdog runs but reboot calls fail gracefully (no real droplet).
DO_API_TOKEN=

# ---------------------------------------------------------------------------
# VRAM budget note for RTX 3070 (8 GB)
# ---------------------------------------------------------------------------
# llama-cpp Q4_K_M 8B:  ~5 GB
# qwen-embeddings:       ~1 GB
# whisper small:         ~1 GB
# chatterbox placeholder:~1 GB
# TOTAL:                 ~8 GB  (tight — reduce LLAMA_PARALLEL if OOM)
#
# If OOM: switch llama to Q4_0 (~4 GB) or reduce LLAMA_GPU_LAYERS to offload
# some layers to CPU (e.g. LLAMA_GPU_LAYERS=28 for 8B model).
