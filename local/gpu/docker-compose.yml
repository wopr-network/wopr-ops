## GPU inner compose stack — runs INSIDE the wopr-gpu DinD container.
##
## Services: llama-cpp, qwen-embeddings, chatterbox, whisper
##
## This is NOT invoked directly from the host. It is started by the
## entrypoint.sh script inside the wopr-gpu container after Docker and
## the NVIDIA Container Toolkit have been installed.
##
## Ports 8080-8083 are published by the outer gpu container to the host.
## The inner services also publish the same ports so platform-api can reach
## them via hostname "wopr-gpu" on the outer bridge network.
##
## Image notes (CRITICAL — validated in RUNBOOK.md):
##   - llama-cpp:      ghcr.io/ggml-org/llama.cpp (NOT ggerganov — that namespace is dead)
##   - chatterbox:     travisvn/chatterbox-tts-api:gpu (NOT :v1.0.1 — that tag is CPU-only)
##   - VRAM budget for RTX 3070 (8 GB):
##       llama Q4_K_M 8B: ~5 GB
##       qwen-embeddings: ~1 GB
##       whisper small:   ~1 GB
##       chatterbox:      ~1 GB
##       TOTAL: ~8 GB (tight — run llm OR voice, not both simultaneously)

x-gpu-common: &gpu-common
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

networks:
  wopr-gpu-local:
    driver: bridge

services:

  # ---------------------------------------------------------------------------
  # llama-cpp — text generation (OpenAI-compatible /v1/chat/completions)
  # ---------------------------------------------------------------------------
  llama-cpp:
    <<: *gpu-common
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: wopr-gpu-llama-cpp
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - /opt/models:/models:ro
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - /models/${LLAMA_MODEL_FILE:-llama.gguf}
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-4096}"
      - --n-gpu-layers
      - "${LLAMA_GPU_LAYERS:-0}"
      - --parallel
      - "${LLAMA_PARALLEL:-2}"
      - --cont-batching
      - --flash-attn
      - "off"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu-local

  # ---------------------------------------------------------------------------
  # qwen-embeddings — embedding generation (OpenAI-compatible /v1/embeddings)
  # ---------------------------------------------------------------------------
  qwen-embeddings:
    <<: *gpu-common
    image: ghcr.io/ggml-org/llama.cpp:server-cuda
    container_name: wopr-gpu-qwen-embeddings
    restart: unless-stopped
    ports:
      - "8083:8080"
    volumes:
      - /opt/models:/models:ro
    command:
      - --host
      - "0.0.0.0"
      - --port
      - "8080"
      - --model
      - /models/${EMBEDDINGS_MODEL_FILE:-qwen2-0_5b-instruct-q8_0.gguf}
      - --ctx-size
      - "${EMBEDDINGS_CTX_SIZE:-8192}"
      - --n-gpu-layers
      - "${EMBEDDINGS_GPU_LAYERS:-0}"
      - --embedding
      - --pooling
      - "${EMBEDDINGS_POOLING:-mean}"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu-local

  # ---------------------------------------------------------------------------
  # chatterbox — text-to-speech (OpenAI-compatible /v1/audio/speech)
  # CRITICAL: use :gpu tag. :v1.0.1 is CPU-only and will be unusably slow.
  # Inner port: 5123. VRAM: ~5 GB on RTX 3070.
  # ---------------------------------------------------------------------------
  chatterbox:
    <<: *gpu-common
    image: travisvn/chatterbox-tts-api:gpu
    container_name: wopr-gpu-chatterbox
    restart: unless-stopped
    ports:
      - "8081:5123"
    environment:
      CHATTERBOX_DEVICE: cuda
    volumes:
      - /opt/voices:/app/voices:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5123/health || curl -sf http://localhost:5123/"]
      interval: 30s
      timeout: 10s
      start_period: 180s
      retries: 3
    networks:
      - wopr-gpu-local

  # ---------------------------------------------------------------------------
  # whisper — speech-to-text (OpenAI-compatible /v1/audio/transcriptions)
  # ---------------------------------------------------------------------------
  whisper:
    <<: *gpu-common
    image: fedirz/faster-whisper-server:0.6.0-rc.3-cuda
    container_name: wopr-gpu-whisper
    restart: unless-stopped
    ports:
      - "8082:8000"
    environment:
      WHISPER__MODEL: ${WHISPER_MODEL:-Systran/faster-whisper-small}
      WHISPER__INFERENCE_DEVICE: cuda
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      start_period: 120s
      retries: 3
    networks:
      - wopr-gpu-local
