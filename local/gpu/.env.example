# GPU machine environment — copy to local/gpu/.env and fill in values.
#
# Usage:
#   cp local/gpu/.env.example local/gpu/.env
#   # Edit local/gpu/.env as needed (most defaults are fine)
#
# This file is bind-mounted read-only into the wopr-gpu container and
# passed to the inner GPU compose stack. Never commit .env (only .env.example).

# ---------------------------------------------------------------------------
# Model weights
# ---------------------------------------------------------------------------
# Host path /opt/models is bind-mounted read-only into /opt/models inside the
# gpu container, and then into /models inside the inner GPU service containers.
# Required files (see RUNBOOK.md GPU section for download instructions):
#   - Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf  (~5 GB VRAM on RTX 3070)
#   - qwen2-0_5b-instruct-q8_0.gguf            (~1 GB VRAM)
# Whisper and Chatterbox models are auto-downloaded on first start.

# llama.cpp text generation
# Q4_K_M recommended for RTX 3070 (8 GB VRAM). Q8_0 (~9 GB) will not fit.
LLAMA_MODEL_FILE=Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
LLAMA_CTX_SIZE=4096
LLAMA_GPU_LAYERS=-1
# Reduce parallel slots on 8 GB VRAM — more slots = more VRAM per slot
LLAMA_PARALLEL=2

# Qwen embeddings
EMBEDDINGS_MODEL_FILE=qwen2-0_5b-instruct-q8_0.gguf
EMBEDDINGS_CTX_SIZE=8192
EMBEDDINGS_POOLING=mean

# Whisper STT (auto-downloaded from HuggingFace on first start)
# Use tiny or small on RTX 3070 to save VRAM
WHISPER_MODEL=Systran/faster-whisper-small

# ---------------------------------------------------------------------------
# VRAM budget note for RTX 3070 (8 GB)
# ---------------------------------------------------------------------------
# llama-cpp Q4_K_M 8B:  ~5 GB
# qwen-embeddings:       ~1 GB
# whisper small:         ~1 GB
# chatterbox:            ~1 GB
# TOTAL:                 ~8 GB (tight — reduce LLAMA_PARALLEL or GPU_LAYERS if OOM)
#
# If OOM: switch llama to Q4_0 (~4 GB), set LLAMA_GPU_LAYERS=28 to offload
# some layers to CPU, or reduce LLAMA_PARALLEL to 1.
